{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1: Using CountVectorizer**"
      ],
      "metadata": {
        "id": "d2akj6Ln6Riu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Define the corpus\n",
        "c = {\n",
        "    'Lincoln1865': 'With malice toward none...',\n",
        "    'TrumpMay26': 'There is NO WAY...',\n",
        "    'Wikipedia': 'In 1998, Oregon became...',\n",
        "    'FortuneMay26': 'Over the last two decades...',\n",
        "    'TheHillApr07': 'Trump voted by mail...',\n",
        "    'KingJamesBible': 'Wherefore laying aside all malice...',\n",
        "}\n",
        "\n",
        "# Create a CountVectorizer instance\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(list(c.values()))\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out(), index=c.keys())\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_h9dAQV4a_N",
        "outputId": "7b5c9ead-cb28-4294-9bb5-9880e263b62a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                1998  all  aside  became  by  decades  in  is  last  laying  \\\n",
            "Lincoln1865        0    0      0       0   0        0   0   0     0       0   \n",
            "TrumpMay26         0    0      0       0   0        0   0   1     0       0   \n",
            "Wikipedia          1    0      0       1   0        0   1   0     0       0   \n",
            "FortuneMay26       0    0      0       0   0        1   0   0     1       0   \n",
            "TheHillApr07       0    0      0       0   1        0   0   0     0       0   \n",
            "KingJamesBible     0    1      1       0   0        0   0   0     0       1   \n",
            "\n",
            "                ...  over  the  there  toward  trump  two  voted  way  \\\n",
            "Lincoln1865     ...     0    0      0       1      0    0      0    0   \n",
            "TrumpMay26      ...     0    0      1       0      0    0      0    1   \n",
            "Wikipedia       ...     0    0      0       0      0    0      0    0   \n",
            "FortuneMay26    ...     1    1      0       0      0    1      0    0   \n",
            "TheHillApr07    ...     0    0      0       0      1    0      1    0   \n",
            "KingJamesBible  ...     0    0      0       0      0    0      0    0   \n",
            "\n",
            "                wherefore  with  \n",
            "Lincoln1865             0     1  \n",
            "TrumpMay26              0     0  \n",
            "Wikipedia               0     0  \n",
            "FortuneMay26            0     0  \n",
            "TheHillApr07            0     0  \n",
            "KingJamesBible          1     0  \n",
            "\n",
            "[6 rows x 25 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2: Using custom tokenizer with CountVectorizer**"
      ],
      "metadata": {
        "id": "zj8CmND-6LW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load Spacy with English model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Custom tokenizer function using lemmatization\n",
        "def custom_tokenizer(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
        "\n",
        "# Create CountVectorizer with custom tokenizer\n",
        "vectorizer_lemma = CountVectorizer(tokenizer=custom_tokenizer)\n",
        "X_lemma = vectorizer_lemma.fit_transform(list(c.values()))\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_lemma = pd.DataFrame(X_lemma.toarray(), columns=vectorizer_lemma.get_feature_names_out(), index=c.keys())\n",
        "print(df_lemma)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bs5IGTly49qA",
        "outputId": "2b81a773-6131-4e36-9fd2-4b328a8e7dba"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                aside  decade  lay  mail  malice  oregon  trump  vote  way  \\\n",
            "Lincoln1865         0       0    0     0       1       0      0     0    0   \n",
            "TrumpMay26          0       0    0     0       0       0      0     0    1   \n",
            "Wikipedia           0       0    0     0       0       1      0     0    0   \n",
            "FortuneMay26        0       1    0     0       0       0      0     0    0   \n",
            "TheHillApr07        0       0    0     1       0       0      1     1    0   \n",
            "KingJamesBible      1       0    1     0       1       0      0     0    0   \n",
            "\n",
            "                wherefore  \n",
            "Lincoln1865             0  \n",
            "TrumpMay26              0  \n",
            "Wikipedia               0  \n",
            "FortuneMay26            0  \n",
            "TheHillApr07            0  \n",
            "KingJamesBible          1  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3: Compute LSA (Latent Semantic Analysis)**"
      ],
      "metadata": {
        "id": "3rmjkzG76COj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# Perform LSA on the term-document matrix from Task 2\n",
        "lsa = TruncatedSVD(n_components=3)\n",
        "lsa.fit(X_lemma)\n",
        "doc_word_lsa = lsa.transform(X_lemma)\n",
        "word_doc_lsa = lsa.components_.T\n",
        "\n",
        "# Vector representation of 'vote'\n",
        "word_index = list(vectorizer_lemma.vocabulary_.keys()).index('vote')\n",
        "vote_representation = word_doc_lsa[word_index]\n",
        "print(\"Vector representation of 'vote':\", vote_representation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpg68cdQ5DFR",
        "outputId": "6a1a7b5b-fd68-4c49-9846-b57a9f7f7e46"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector representation of 'vote': [ 1.35704883e-16  6.36748730e-17 -4.82774442e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 4: Compute cosine similarity**"
      ],
      "metadata": {
        "id": "ccqklP3v57lZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Compute cosine similarity between 'malice' and 'vote'\n",
        "cosine_malice_vote = cosine_similarity(word_doc_lsa[list(vectorizer_lemma.vocabulary_).index('malice')].reshape(1, -1),\n",
        "                                       word_doc_lsa[list(vectorizer_lemma.vocabulary_).index('vote')].reshape(1, -1))\n",
        "\n",
        "# Compute cosine similarity between 'mail' and 'vote'\n",
        "cosine_mail_vote = cosine_similarity(word_doc_lsa[list(vectorizer_lemma.vocabulary_).index('mail')].reshape(1, -1),\n",
        "                                     word_doc_lsa[list(vectorizer_lemma.vocabulary_).index('vote')].reshape(1, -1))\n",
        "\n",
        "print(\"Cosine similarity between 'malice' and 'vote':\", cosine_malice_vote)\n",
        "print(\"Cosine similarity between 'mail' and 'vote':\", cosine_mail_vote)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S24yG0rI5II-",
        "outputId": "c3e85efa-5964-45f8-9cf2-36aaf5813c39"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between 'malice' and 'vote': [[3.37349916e-16]]\n",
            "Cosine similarity between 'mail' and 'vote': [[1.54311775e-16]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 5: Compute TF-IDF matrix using TfidfVectorizer**"
      ],
      "metadata": {
        "id": "awQnHNnS5zWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(list(c.values()))\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out(), index=c.keys())\n",
        "print(df_tfidf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFbVbCmn5NiP",
        "outputId": "7590eae6-ca05-4275-ee7c-c085ce65e3e8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   aside  decade       lay     mail    malice  oregon  \\\n",
            "Lincoln1865     0.000000     0.0  0.000000  0.00000  1.000000     0.0   \n",
            "TrumpMay26      0.000000     0.0  0.000000  0.00000  0.000000     0.0   \n",
            "Wikipedia       0.000000     0.0  0.000000  0.00000  0.000000     1.0   \n",
            "FortuneMay26    0.000000     1.0  0.000000  0.00000  0.000000     0.0   \n",
            "TheHillApr07    0.000000     0.0  0.000000  0.57735  0.000000     0.0   \n",
            "KingJamesBible  0.521823     0.0  0.521823  0.00000  0.427903     0.0   \n",
            "\n",
            "                  trump     vote  way  wherefore  \n",
            "Lincoln1865     0.00000  0.00000  0.0   0.000000  \n",
            "TrumpMay26      0.00000  0.00000  1.0   0.000000  \n",
            "Wikipedia       0.00000  0.00000  0.0   0.000000  \n",
            "FortuneMay26    0.00000  0.00000  0.0   0.000000  \n",
            "TheHillApr07    0.57735  0.57735  0.0   0.000000  \n",
            "KingJamesBible  0.00000  0.00000  0.0   0.521823  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 6: Compute cosine similarity using TF-IDF matrix**"
      ],
      "metadata": {
        "id": "0tR4lceL5wCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Compute cosine similarity between 'malice' and 'vote' using TF-IDF matrix\n",
        "cosine_malice_vote_tfidf = cosine_similarity(X_tfidf[:, vectorizer_lemma.vocabulary_['malice']].reshape(1, -1),\n",
        "                                             X_tfidf[:, vectorizer_lemma.vocabulary_['vote']].reshape(1, -1))\n",
        "\n",
        "# Compute cosine similarity between 'mail' and 'vote' using TF-IDF matrix\n",
        "cosine_mail_vote_tfidf = cosine_similarity(X_tfidf[:, vectorizer_lemma.vocabulary_['mail']].reshape(1, -1),\n",
        "                                           X_tfidf[:, vectorizer_lemma.vocabulary_['vote']].reshape(1, -1))\n",
        "\n",
        "print(\"Cosine similarity between 'malice' and 'vote' (TF-IDF):\", cosine_malice_vote_tfidf)\n",
        "print(\"Cosine similarity between 'mail' and 'vote' (TF-IDF):\", cosine_mail_vote_tfidf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxWDGHYL5S3D",
        "outputId": "31f4b9d9-b4e2-4f02-e3d3-1374e4142c2e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between 'malice' and 'vote' (TF-IDF): [[0.]]\n",
            "Cosine similarity between 'mail' and 'vote' (TF-IDF): [[1.]]\n"
          ]
        }
      ]
    }
  ]
}